<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>400c953cbde04a418e137e1b49fce861</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown" id="Vi44XnleWZXu">
<p>“What I cannot create, I do not understand.”</p>
<p>—Richard Feynman</p>
</div>
<div class="cell markdown" id="432B8cUkfpc2">
<p>Thanks to high powered graphics cards and neural networks, image classification is now more or less a solved problem. But many machine learning enthusiasts don't really understand what's going on under the hood. I decided to create this tutorial on how to code a complete image classification neural network from scratch, because until you can take something apart and put it back together again, you don't truly understand it.</p>
</div>
<div class="cell markdown" id="FyAHfO_blTOO">
<p>While the below steps could technically done with no imports, numpy will make things a lot easier. Neural networks are only fast when they make use of c-like arrays, which python does not have natively.</p>
</div>
<div class="cell code" data-execution_count="326" id="PPOXA1zhf5ce">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="cell markdown" id="kWgXpeO5lxaO">
<p>Ok, so what is a neural network? At its core, it's an acyclic directed graph of neurons, and those neurons are simply nonlinear regression functions. The equation for a line is:</p>
<p>y = mx + b</p>
<p>When talking about neural nets, we call the slope weights and the y-intercept bias, so it becomes:</p>
<p>y = wx + b</p>
<p>but this equation assumes that w and x are scalars. If they're vectors, we want to multiply each weight times each x value and then add them together. This is called the dot product, represented as</p>
<p>w⋅x</p>
<p>in order to turn this into nonlinear regression, we need to introduce nonlinearity with an activation function. A popular one in neural networks is the sigmoid function, so we'll use that:</p>
<p>y = sigmoid(w⋅x + b)</p>
</div>
<div class="cell markdown" id="7ju-J4r6pwqm">
<p>The sigmoid function is defined like this</p>
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/b85a44504d35ed1d51b921afb4f8d8ad87949966.png" alt="image.png" /></p>
</div>
<div class="cell markdown" id="8VlgkEc0qHvp">
<p>so let's put this into code, since we know we'll need it.</p>
</div>
<div class="cell code" data-execution_count="327" id="-3-PBr1dn93I">
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>z))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Another perk of using numpy is that it has a built in function for raising euler&#39;s number to a power</span></span></code></pre></div>
</div>
<div class="cell markdown" id="jhbyIAlvrwGA">
<p>We're also going to need a loss function to determine how badly to punish wrong answers. A common one used is Mean Squared error</p>
</div>
<div class="cell code" data-execution_count="328" id="2rm7fpRdskLd">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> MSE_loss(real, predicted):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>(real <span class="op">-</span> predicted)<span class="op">**</span><span class="dv">2</span></span></code></pre></div>
</div>
<div class="cell markdown" id="3eDU4bXDqy4Q">
<p>Now let's make our neuron class. In my first attempt at this, I initially had a structure like this</p>
<p><br></p>
<p>neural network</p>
<p>contains layers</p>
<p>contains neurons</p>
<p>contains <strong>weights</strong> and a <strong>bias</strong></p>
<p><br></p>
<p>If you do it this way, the weights are a vector, the input is a vector, and the bias is a scalar. However, I quickly realized why pytorch and tensorflow don't do it this way. It's faster and cleaner to use a structure like this:</p>
<p><br></p>
<p>neural network</p>
<p>contains layers</p>
<p>contains a <strong>weight matrix</strong> of shape (number_of_neurons, number_of_weights) and a <strong>bias vector</strong> of length (number_of_neurons)</p>
</div>
<div class="cell markdown" id="YttzC2xquum0">
<p>Essentially all we've done is just pack the neurons together.</p>
<p>If we do it this way, we can multiply a matrix of neurons times a vector of inputs and parallelize our forward and backward pass. Essentially every neuron can get calculated at the same time using a GPU rather than one at a time with a CPU.</p>
<p>Before, each neuron had a vector of weights and a scalar bias.</p>
<p>Now, each layer will have a matrix of weight vectors and a vector of biases.</p>
<p>Since neurons are now just rows in a matrix, we don't need a neuron class, so let's define our Layer class that will contain all the neurons:</p>
</div>
<div class="cell code" data-execution_count="329" id="sHso0M4-qcrI">
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Layer:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,num_neurons, input_length):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is the number of neurons in this layer</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_neurons <span class="op">=</span> num_neurons</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is the number of scalar values in the input vector</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_length <span class="op">=</span> input_length</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># note that weights and biases are initialized as random values and will become the right values after backpropagation</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># each neuron has as many weighs as there are input values</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so the vector of weights is now a matrix of all the neuron weights in the layer</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.random.rand(num_neurons, input_length) <span class="op">-</span> <span class="fl">1.0</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># there is always one bias per neuron, so biases will be a vector of length num_neurons</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.random.rand(num_neurons) <span class="op">-</span> <span class="fl">1.0</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the forward function will pass an input vector to every neuron</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># then it calculates every neuron&#39;s output at once and outputs them as a vector</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid(np.dot(<span class="va">self</span>.weights, x) <span class="op">+</span> <span class="va">self</span>.biases)</span></code></pre></div>
</div>
<div class="cell markdown" id="59LP40HTjdGg">
<p>Now let's define our neural network class. All it needs to do in the forward pass is feed the input into the first layer, and then feed each layer's output into the next layer. For this example we'll define our network as having 3 hidden layer neurons and one output neuron for simplicity.</p>
</div>
<div class="cell code" data-execution_count="330" id="sWMAZgmYj7t5">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,input_length):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the hidden layer is composed of three neurons</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># each neuron will have as many weights as there are values in the input</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> Layer(<span class="dv">3</span> ,input_length)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The outpupt layer will have a single neuron</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># it will have three weights, each one corresponding to the outputs of the three previous layer&#39;s neurons </span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> Layer(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this learning rate hyperparameter will be important when we implement backpropagation</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Putting the layers in a list lets us loop through them, while having them as variables still lets us hard-code things</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> [</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hidden_layer,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.output_layer</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this function will pass the input through all the layers of neurons</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we&#39;ll need to store every layer&#39;s output because these values are used to teach each layer what it did wrong in backpropagation</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        output_vectors <span class="op">=</span> []</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loop through all of the layers, starting with the hidden layer</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if we&#39;re on the first layer by checking if output_vectors is still empty</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> output_vectors:</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                <span class="co"># pass the first layer the input vector</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> layer.forward(x)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>                <span class="co"># otherwise, if it&#39;s not the first layer, pass it the previous layer&#39;s output, which will be the last value in output_vectors</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> layer.forward(output_vectors[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the result of this layer&#39;s forward pass to the list of outputs</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>            output_vectors.append(output)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return these outputs for use in prediction and backpropagation</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output_vectors</span></code></pre></div>
</div>
<div class="cell markdown" id="q9rI81FSlove">
<p>Now, believe it or not, we've actually built a neural network. It's incapable of learning, because we haven't implemented backpropagation yet, but let's pass something through it and see what happens.</p>
<p>When I first started this project I wanted to get right to image classification, but before we get to that we should start with a toy example. The XOR problem is the simplest way to check whether a neural network is capable of solving nonlinearly seperable problems is with the following dataset:</p>
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/5fad36555be78593a0706c690222f2b07ab3036d.png" alt="image.png" /></p>
</div>
<div class="cell code" data-execution_count="331" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Aq8GnZiEnG47" data-outputId="80ca74cd-4b29-404b-fa86-bb9cd5a1d079">
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is the above data, expressed as a list of tuples in the form (input vector of length 2, correct class label)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>xor_data <span class="op">=</span> [([<span class="dv">0</span>, <span class="dv">0</span>], <span class="dv">0</span>), ([<span class="dv">0</span>, <span class="dv">1</span>], <span class="dv">1</span>), ([<span class="dv">1</span>, <span class="dv">0</span>], <span class="dv">1</span>), ([<span class="dv">1</span>, <span class="dv">1</span>], <span class="dv">0</span>)]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create a neural network instance with an input length of 2 for the 3 hidden layer neurons</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>nn <span class="op">=</span> NeuralNetwork(<span class="dv">2</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># iterate through each input and class label in the data</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> input_vector, class_label <span class="kw">in</span> xor_data:</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># pass the input vector to the neural net and collect the output</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  output_vectors <span class="op">=</span> nn.forward(input_vector)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the output of the last layer is the neural network&#39;s prediction</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  prediction <span class="op">=</span> output_vectors[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calculate the loss</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> MSE_loss(class_label, prediction)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f&quot;input vector: </span><span class="sc">{</span>input_vector<span class="sc">}</span><span class="ss">     Correct class label: </span><span class="sc">{</span>class_label<span class="sc">}</span><span class="ss">     neural network predicted class label: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">     loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>  </span></code></pre></div>
<div class="output stream stdout">
<pre><code>input vector: [0, 0]     Correct class label: 0     neural network predicted class label: [0.70491526]     loss: [0.24845276]
input vector: [0, 1]     Correct class label: 1     neural network predicted class label: [0.69251561]     loss: [0.04727333]
input vector: [1, 0]     Correct class label: 1     neural network predicted class label: [0.68950269]     loss: [0.04820429]
input vector: [1, 1]     Correct class label: 0     neural network predicted class label: [0.68072358]     loss: [0.2316923]
</code></pre>
</div>
</div>
<div class="cell markdown" id="dND2AY37pZHd">
<p>Because the weights and biases were initialized randomly and we haven't trained it at all, the outputs should be more or less random as well. Try runninig the above cell multiple times to see this demonstrated.</p>
</div>
<div class="cell markdown" id="fnR1k_Etp6mz">
<p>Now it's time for the hardest part. Let's implement backpropagation. The first thing we want to find is how much the loss changes when we change the weights and biases. This is called the delta, and it's found by taking the derivative of the loss with respect to the weights.</p>
<p>If our function looks like this, where z is the dot product of the weights and the inputs plus the bias, and σ is the sigmoid function:</p>
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/c29c4d2b7e168d57617bbd4b212ea74c377a8193.png" alt="image.png" /></p>
<p>and our loss function is mean squared error, then the derivative of the loss with respect to the weights is calculated as follows, using chain rule to multiply the derivatives of the nested functions:</p>
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/d56fcc75599966da8c31827fa9e90092c67dab8c.png" alt="image.png" /></p>
</div>
<div class="cell markdown" id="lkk0bqgQ0lkB">
<p>In words, this is</p>
<p><br></p>
<p>Derivative of error w.r.t weights =</p>
<p><br></p>
<p>( derivative of error w.r.t sigmoid *</p>
<p>derivative of sigmoid w.r.t. (the dot product + bias) *</p>
<p>derivative of (the dot product + bais) w.r.t. weights )</p>
</div>
<div class="cell markdown" id="dQDbFNIx2ROT">
<p>Taking each derivative individually:</p>
<p><br></p>
<p>dE/dsigmoid = <font color='firebrick'>output - ground_truth<font></p>
<p>dsigmoid/dz = <font color='blue'>output * (1 - output)<font></p>
<p>dz/dw = <font color='green'>input</p>
</div>
<div class="cell markdown" id="Lv_CKk4x3X0g">
<p>This gives us the formula to calculate the delta for the output neuron: <br></p>
<p>output_delta = <font color='firebrick'>(output - ground_truth)<font> * <font color='blue'>output<font> <em>(1 - output) </em> <font color='green'>input<font></p>
</div>
<div class="cell markdown" id="gNg-3ZjM4147">
<p>but what about all the other neurons?</p>
<p>The derivative of the hidden layer depends on the derivative of the output layer, so we can't just calculate it the same way. Instead let's think about it like a nested function, and then use chain rule to get:</p>
</div>
<div class="cell markdown" id="N2ygeb-n54p_">
<p>Given neuron 1 -&gt; neuron 2 -&gt; neuron 3</p>
<pre><code>			Neuron 3 delta =           dE/dw3
	  Neuron 2 delta = dE/dw2 =        d3w/d2w * dE/dw3
Neuron 1 delta = dE/dw1 =		      d2w/d1w * d3w/d2w * dE/dw3</code></pre>
</div>
<div class="cell markdown" id="x_UHzvIw6uSw">
<p>In other words, each delta uses the same formula as above, but because of chain rule we multiply it times the gradients of every layer that comes after it. Implementing backpropagation means writing an algorithm to iteratively calculate these derivatives before using them to add and subtract from the weights.</p>
<p>note also that the derivative of the error with respect to the bias will be slightly different. We calculated the weights using the derivative of the dot product plus the bias with respect to the weights, like this:</p>
<p><br></p>
<p>dz/dw = input</p>
<p><br></p>
<p>But for the bias we need the derivative of the dot product plus the bias with respect to the bias. Since the derivative of a constant is 1, we get:</p>
<p><br></p>
<p>dz/db = 1</p>
<p><br></p>
<p>this means that instead of this formula</p>
<p>output_weight_delta = <font color='firebrick'>(output - ground_truth)<font> * <font color='blue'>output<font> <em>(1 - output) </em> <font color='green'>input<font></p>
<p><br></p>
<p>we get this one:</p>
<p>output_bias_delta = <font color='firebrick'>(output - ground_truth)<font> * <font color='blue'>output<font> <em>(1 - output) </em> <font color='green'>1<font></p>
</div>
<div class="cell markdown" id="ZLY2IpNHAVC0">
<p>Ok, now that all the math is out of the way, we're ready to put it into code: (the below code is just for demonstration, it won't do anything until we put it into the neural network class)</p>
</div>
<div class="cell code" data-execution_count="332" id="SvLd4CO5AmVI">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backpropagate(<span class="va">self</span>, output, y):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is the dError/dSigmoid</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    E1 <span class="op">=</span> output[<span class="dv">2</span>] <span class="op">-</span> y</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is the output bias delta, used to update the output neuron bias</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    dW2 <span class="op">=</span> E1 <span class="op">*</span> output[<span class="dv">2</span>] <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>output[<span class="dv">2</span>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is dweights_hidden/dweights_output</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    E2 <span class="op">=</span> np.dot(np.reshape(dW2, (<span class="dv">1</span>,<span class="dv">1</span>)), <span class="va">self</span>.output_layer.weights)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is hidden layer bias delta, used to update the hidden layer neuron biases</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    dW1 <span class="op">=</span> E2 <span class="op">*</span> output[<span class="dv">1</span>] <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>output[<span class="dv">1</span>])</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># now we multiply times the input to get the weights deltas</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is the output weights delta, used to update the output neuron weights</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    W2_update <span class="op">=</span> np.dot(np.reshape(output[<span class="dv">1</span>], (<span class="dv">3</span>,<span class="dv">1</span>)), np.reshape(dW2, (<span class="dv">1</span>,<span class="dv">1</span>)))</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is the hidden weights delta, used to update the hidden layer neuron weights</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    W1_update <span class="op">=</span> np.dot(np.reshape(output[<span class="dv">0</span>], (<span class="dv">2</span>,<span class="dv">1</span>)), dW1)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># new biases = old biases - (learning rate * bias delta)</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.output_layer.biases <span class="op">=</span> <span class="va">self</span>.output_layer.biases <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> dW2</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.output_layer.weights <span class="op">=</span> <span class="va">self</span>.output_layer.weights <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> W2_update.T</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># new weights = old weights - (learning rate * weights delta)</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.hidden_layer.biases <span class="op">=</span> <span class="va">self</span>.output_layer.biases <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> np.reshape(dW1,(<span class="dv">3</span>,))</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.hidden_layer.weights <span class="op">=</span> <span class="va">self</span>.hidden_layer.weights <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> W1_update.T</span></code></pre></div>
</div>
<div class="cell markdown" id="cc3JOnkCZvLU">
<p>Let's put our code all together and see how it does on the xor problem!</p>
</div>
<div class="cell code" data-execution_count="333" id="smEEWEKKZ0l0">
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>z))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> derivative_of_sigmoid(z):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(z)<span class="op">/</span>((np.exp(z) <span class="op">+</span> <span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> L2_loss(real, predicted):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>(real <span class="op">-</span> predicted)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Layer:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,num_neurons, input_length):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_neurons <span class="op">=</span> num_neurons</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_length <span class="op">=</span> input_length</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.random.rand(num_neurons, input_length) <span class="op">-</span> <span class="fl">1.0</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.random.rand(num_neurons) <span class="op">-</span> <span class="fl">1.0</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid(np.dot(<span class="va">self</span>.weights, x) <span class="op">+</span> <span class="va">self</span>.biases)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork:</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,input_length, num_hidden_layer_neurons <span class="op">=</span> <span class="dv">3</span>):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_length <span class="op">=</span> input_length</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> Layer(num_hidden_layer_neurons,<span class="va">self</span>.input_length)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> Layer(<span class="dv">1</span>,num_hidden_layer_neurons)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> [</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hidden_layer,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.output_layer</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        output_vectors <span class="op">=</span> []</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> output_vectors:</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> layer.forward(x)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> layer.forward(output_vectors[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>            output_vectors.append(output)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output_vectors</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backpropagate(<span class="va">self</span>, output, y):</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        E1 <span class="op">=</span> output[<span class="dv">2</span>] <span class="op">-</span> y</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        dW2 <span class="op">=</span> E1 <span class="op">*</span> output[<span class="dv">2</span>] <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>output[<span class="dv">2</span>])</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        E2 <span class="op">=</span> np.dot(np.reshape(dW2, (<span class="dv">1</span>,<span class="dv">1</span>)), <span class="va">self</span>.output_layer.weights)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        dW1 <span class="op">=</span> E2 <span class="op">*</span> output[<span class="dv">1</span>] <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>output[<span class="dv">1</span>])</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        W2_update <span class="op">=</span> np.dot(np.reshape(output[<span class="dv">1</span>], (<span class="va">self</span>.hidden_layer.num_neurons,<span class="dv">1</span>)), np.reshape(dW2, (<span class="dv">1</span>,<span class="dv">1</span>)))</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        W1_update <span class="op">=</span> np.dot(np.reshape(output[<span class="dv">0</span>], (<span class="va">self</span>.input_length,<span class="dv">1</span>)), dW1)</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer.biases <span class="op">=</span> <span class="va">self</span>.output_layer.biases <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> dW2</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer.weights <span class="op">=</span> <span class="va">self</span>.output_layer.weights <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> W2_update.T</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer.biases <span class="op">=</span> <span class="va">self</span>.output_layer.biases <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> np.reshape(dW1,(<span class="va">self</span>.hidden_layer.num_neurons,))</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer.weights <span class="op">=</span> <span class="va">self</span>.hidden_layer.weights <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> W1_update.T</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="334" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="BHRMXEOag9pq" data-outputId="aedf4cf7-e5b9-4cdf-87a7-ed0bcf029fe6">
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>input_length <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>nn <span class="op">=</span> NeuralNetwork(input_length)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [([<span class="dv">0</span>, <span class="dv">0</span>], <span class="dv">0</span>), ([<span class="dv">0</span>, <span class="dv">1</span>], <span class="dv">1</span>), ([<span class="dv">1</span>, <span class="dv">0</span>], <span class="dv">1</span>), ([<span class="dv">1</span>, <span class="dv">1</span>], <span class="dv">0</span>)]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;BEFORE ANY TRAINING&quot;</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;INPUT : GROUND TRUTH : PREDICTIONS&quot;</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> input_vector, class_label <span class="kw">in</span> data:</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    input_vector <span class="op">=</span> np.array(input_vector)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    output_vectors <span class="op">=</span> nn.forward(input_vector)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    output_vectors.insert(<span class="dv">0</span>,input_vector)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    nn.backpropagate(output_vectors, class_label)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> nn.forward(input_vector)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> L2_loss(class_label, prediction)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">str</span>(input_vector) <span class="op">+</span><span class="st">&quot; : &quot;</span> <span class="op">+</span> <span class="bu">str</span>(class_label) <span class="op">+</span> <span class="st">&quot; : &quot;</span><span class="op">+</span> <span class="bu">str</span>(prediction[<span class="dv">0</span>]))</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(epoch):</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> input_vector, class_label <span class="kw">in</span> data:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        input_vector <span class="op">=</span> np.array(input_vector)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        output_vectors <span class="op">=</span> nn.forward(input_vector)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        output_vectors.insert(<span class="dv">0</span>,input_vector)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        nn.backpropagate(output_vectors, class_label)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> nn.forward(input_vector)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> L2_loss(class_label, prediction)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="bu">str</span>(input_vector) <span class="op">+</span><span class="st">&quot; : &quot;</span> <span class="op">+</span> <span class="bu">str</span>(class_label) <span class="op">+</span> <span class="st">&quot; : &quot;</span><span class="op">+</span> <span class="bu">str</span>(prediction[<span class="dv">0</span>]))</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="ss">f&quot;EPOCH </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">&quot;INPUT : GROUND TRUTH : PREDICTIONS&quot;</span>)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    train(epoch)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>BEFORE ANY TRAINING
INPUT : GROUND TRUTH : PREDICTIONS
[0 0] : 0 : 0.5372297921521297
[0 1] : 1 : 0.619296178746772
[1 0] : 1 : 0.6726602721261201
[1 1] : 0 : 0.6360693271935756


EPOCH 0
INPUT : GROUND TRUTH : PREDICTIONS
[0 0] : 0 : 0.4923760826647501
[0 1] : 1 : 0.5825236905915935
[1 0] : 1 : 0.6385282966821895
[1 1] : 0 : 0.6050730424842784


EPOCH 1000
INPUT : GROUND TRUTH : PREDICTIONS
[0 0] : 0 : 0.06909748221466881
[0 1] : 1 : 0.9288672118634397
[1 0] : 1 : 0.9278121869481338
[1 1] : 0 : 0.08869306636542415


EPOCH 2000
INPUT : GROUND TRUTH : PREDICTIONS
[0 0] : 0 : 0.030642525288186038
[0 1] : 1 : 0.9680033344706835
[1 0] : 1 : 0.9678420521957557
[1 1] : 0 : 0.03954630476494741


EPOCH 3000
INPUT : GROUND TRUTH : PREDICTIONS
[0 0] : 0 : 0.02234435891832155
[0 1] : 1 : 0.9765592046970996
[1 0] : 1 : 0.9764785866145611
[1 1] : 0 : 0.02901934514225483


EPOCH 4000
INPUT : GROUND TRUTH : PREDICTIONS
[0 0] : 0 : 0.01834962392745754
[0 1] : 1 : 0.9807048039800075
[1 0] : 1 : 0.9806519883785018
[1 1] : 0 : 0.02391905956321031


</code></pre>
</div>
</div>
<div class="cell markdown" id="Afp01qLvf-Qe">
<p>As you can see, the guesses start more or less random, but as the neural network trains, the predictions for 0 inch closer and closer to 0, and the predictions for 1 inch closer and closer to 1. After thousands of epochs, the predictions are very close to the ground truth.</p>
<p><br></p>
<p>Now that we know it works, let's try it on images!</p>
</div>
<div class="cell code" data-execution_count="335" data-colab="{&quot;height&quot;:409,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="pbJ1iQC4Pmja" data-outputId="1f7e952d-8a81-4534-977e-7c134eab0c60">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>apple_train_link <span class="op">=</span> <span class="st">&quot;https://drive.google.com/drive/folders/1j-M596K7252N40ntYQOCMVjpASLFco9M?usp=share_link&quot;</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>banana_train_link <span class="op">=</span> <span class="st">&quot;https://drive.google.com/drive/folders/1jZvNwVZDPN6Q52nY6cu9hj_p3jOeomVt?usp=share_link&quot;</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>git clone https:<span class="op">//</span>github.com<span class="op">/</span>alexjbusch<span class="op">/</span>fruit_classification_from_scratch</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>banana_path <span class="op">=</span> <span class="st">&quot;/content/fruit_classification_from_scratch/banana_train&quot;</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>apple_path <span class="op">=</span> <span class="st">&quot;/content/fruit_classification_from_scratch/apple_train&quot;</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>bananas <span class="op">=</span> os.listdir(banana_path)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>apples <span class="op">=</span> os.listdir(apple_path)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> apple_path<span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>apples[<span class="dv">0</span>]</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;CLASS 0&quot;</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Apple image examples:&quot;</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> apple_path<span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>apples[<span class="dv">100</span>]</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> apple_path<span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>apples[<span class="dv">300</span>]</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;CLASS 1&quot;</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> banana_path<span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span> bananas[<span class="dv">0</span>]</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Banana image examples&quot;</span>)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> banana_path<span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span> bananas[<span class="dv">100</span>]</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> banana_path<span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span> bananas[<span class="dv">120</span>]</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_image(class_label,index):</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> class_label <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> banana_path <span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span> bananas[index]</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> class_label <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> apple_path <span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span> apples[index]</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>  image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(path))</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>  plt.imshow(image)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>fatal: destination path &#39;fruit_classification_from_scratch&#39; already exists and is not an empty directory.


CLASS 0
Apple image examples:
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/7bdbf3fa49cbbec2226ca5643396ae120d47429e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>CLASS 1
Banana image examples
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/29553425500ef227b0c7a1f5038b16003782b1cc.png" /></p>
</div>
</div>
<div class="cell markdown" id="6H27fJmMTWEU">
<p>For a simple classification task, I decided to use images of fruit from this dataset, called Fruits 360: <a href="https://www.kaggle.com/datasets/moltean/fruits" class="uri">https://www.kaggle.com/datasets/moltean/fruits</a></p>
<p>The above six images are examples from the Apple Braeburn and Banana folders in the dataset. There are 492 apples and 490 bananas, and all of them are square 100x100 images. Although the images have color, for simplicity we will grayscale them before passing them through the neural net.</p>
<p><br></p>
<p>Now that we have our images, we'll randomly set aside 10% of them as a testing dataset and train on the rest.</p>
</div>
<div class="cell code" data-execution_count="336" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="lWeO8pfoiQnP" data-outputId="5a32cbf0-19cb-486f-fa65-da7469c253c3">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_test_set(apple_train, banana_train):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f&quot;Sequestering </span><span class="sc">{</span><span class="bu">int</span>(<span class="bu">len</span>(bananas)<span class="op">/</span><span class="dv">10</span>)<span class="op">*</span><span class="dv">2</span><span class="sc">}</span><span class="ss"> pictures for testing&quot;</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  test_samples <span class="op">=</span> random.sample(apples,<span class="bu">int</span>(<span class="bu">len</span>(bananas)<span class="op">/</span><span class="dv">10</span>))</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  apple_test_indicies <span class="op">=</span> [apples.index(sample) <span class="cf">for</span> sample <span class="kw">in</span> test_samples]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  test_samples <span class="op">=</span> random.sample(bananas,<span class="bu">int</span>(<span class="bu">len</span>(apples)<span class="op">/</span><span class="dv">10</span>))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  banana_test_indicies <span class="op">=</span> [bananas.index(sample) <span class="cf">for</span> sample <span class="kw">in</span> test_samples]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> apple_test_indicies, banana_test_indicies</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>apple_test, banana_test <span class="op">=</span> generate_test_set(apples,bananas)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>nn <span class="op">=</span> NeuralNetwork(<span class="dv">10000</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;[Apple class: apple guess, Banana class: banana guess&quot;</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train():</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    image_pair <span class="op">=</span> [<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>]</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    image_class <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    image_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> image_index <span class="op">&lt;</span> <span class="bu">len</span>(bananas):</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image_index <span class="kw">in</span> apple_test:</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>          image_class <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>          <span class="co">#print(&quot;in apple&quot;)</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> image_index <span class="kw">in</span> banana_test:</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>          image_class <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>          image_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>          <span class="cf">continue</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>          <span class="co">#print(&quot;in banana&quot;)</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> image_index <span class="kw">in</span> apple_test <span class="kw">and</span> image_index <span class="kw">in</span> banana_test:</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>          image_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>          <span class="co">#print(&quot;in both&quot;)</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>          <span class="cf">continue</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image_class <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>            image_path <span class="op">=</span> apple_path <span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>apples[image_index]</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(image_path).convert(<span class="st">&#39;L&#39;</span>))</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>            input_vector <span class="op">=</span> image.flatten()</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> image_class <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>            image_path <span class="op">=</span> banana_path <span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>bananas[image_index]</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(image_path).convert(<span class="st">&#39;L&#39;</span>))</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>            input_vector <span class="op">=</span> image.flatten()</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>            image_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        input_vector <span class="op">=</span> np.array(input_vector)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        output_vectors <span class="op">=</span> nn.forward(input_vector)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>        output_vectors.insert(<span class="dv">0</span>,input_vector)</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> output_vectors[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> L2_loss(image_class, prediction)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>        nn.backpropagate(output_vectors, image_class)</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> nn.forward(input_vector)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> L2_loss(image_class, prediction)</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image_index <span class="op">%</span> <span class="dv">300</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> image_class <span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>                image_pair[<span class="dv">0</span>] <span class="op">=</span> <span class="bu">str</span>(image_class) <span class="op">+</span> <span class="st">&quot; : &quot;</span><span class="op">+</span> <span class="bu">str</span>(prediction[<span class="dv">0</span>])</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> image_class <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>                image_pair[<span class="dv">1</span>] <span class="op">=</span> <span class="bu">str</span>(image_class) <span class="op">+</span> <span class="st">&quot; : &quot;</span><span class="op">+</span> <span class="bu">str</span>(prediction[<span class="dv">0</span>])</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> image_pair[<span class="dv">0</span>] <span class="kw">and</span> image_pair[<span class="dv">1</span>]:</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(image_pair)</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>                image_pair <span class="op">=</span> [<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>]</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image_class <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>          image_class <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> image_class <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>          image_class <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>    train()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Sequestering 98 pictures for testing
[Apple class: apple guess, Banana class: banana guess
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-333-2a998eaf377d&gt;:3: RuntimeWarning: overflow encountered in exp
  return 1/(1+np.exp(-z))
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>[&#39;0 : 0.6857854842340634&#39;, &#39;1 : 0.7360188579029986&#39;]
[&#39;0 : 0.01375763024552306&#39;, &#39;1 : 0.7622140779806112&#39;]
[&#39;0 : 0.005387942536129833&#39;, &#39;1 : 0.7843106346240919&#39;]
[&#39;0 : 0.0030977610513754535&#39;, &#39;1 : 0.8007851129713525&#39;]
[&#39;0 : 0.0020900585943445468&#39;, &#39;1 : 0.8135184778542681&#39;]
[&#39;0 : 0.001541120309295966&#39;, &#39;1 : 0.8237533361202344&#39;]
[&#39;0 : 0.001203184507614878&#39;, &#39;1 : 0.8322235963878937&#39;]
[&#39;0 : 0.0009777614272982404&#39;, &#39;1 : 0.8393869231411691&#39;]
[&#39;0 : 0.0008185331286306812&#39;, &#39;1 : 0.8455468972788835&#39;]
[&#39;0 : 0.0007011039902541462&#39;, &#39;1 : 0.8509152207176829&#39;]
</code></pre>
</div>
</div>
<div class="cell markdown" id="yRBSIhO2IiV7">
<p>The above output shows a guess for the same apple/banana pair once per epoch (which trains on every image in the dataset). The class code for apple is 1. The class code for banana is 0.</p>
<p><br></p>
<p>Unsurprisingly, the results aren't great with one hidden layer of three neurons after 10 epochs. Let's see how it does on the testing data.</p>
</div>
<div class="cell code" data-execution_count="337" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="eE29x7vY7sXx" data-outputId="0aa54d1b-8653-4e71-bf81-ae20928632f3">
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pretty<span class="op">-</span>confusion<span class="op">-</span>matrix</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pretty_confusion_matrix <span class="im">import</span> pp_matrix_from_data</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: pretty-confusion-matrix in /usr/local/lib/python3.8/dist-packages (0.1.1)
Requirement already satisfied: flake8&lt;4.0.0,&gt;=3.9.2 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (3.9.2)
Requirement already satisfied: isort&lt;6.0.0,&gt;=5.8.0 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (5.11.2)
Requirement already satisfied: sklearn&lt;0.1,&gt;=0.0 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (0.0.post1)
Requirement already satisfied: pre-commit&lt;3.0.0,&gt;=2.12.1 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (2.20.0)
Requirement already satisfied: seaborn&lt;0.12.0,&gt;=0.11.2 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (0.11.2)
Requirement already satisfied: black&lt;22.0,&gt;=21.5b0 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (21.12b0)
Requirement already satisfied: pandas&lt;2.0.0,&gt;=1.3.4 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (1.3.5)
Requirement already satisfied: numpy&lt;2.0.0,&gt;=1.21.4 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (1.21.6)
Requirement already satisfied: matplotlib&lt;4.0.0,&gt;=3.5.0 in /usr/local/lib/python3.8/dist-packages (from pretty-confusion-matrix) (3.6.2)
Requirement already satisfied: tomli&lt;2.0.0,&gt;=0.2.6 in /usr/local/lib/python3.8/dist-packages (from black&lt;22.0,&gt;=21.5b0-&gt;pretty-confusion-matrix) (1.2.3)
Requirement already satisfied: pathspec&lt;1,&gt;=0.9.0 in /usr/local/lib/python3.8/dist-packages (from black&lt;22.0,&gt;=21.5b0-&gt;pretty-confusion-matrix) (0.10.3)
Requirement already satisfied: click&gt;=7.1.2 in /usr/local/lib/python3.8/dist-packages (from black&lt;22.0,&gt;=21.5b0-&gt;pretty-confusion-matrix) (7.1.2)
Requirement already satisfied: mypy-extensions&gt;=0.4.3 in /usr/local/lib/python3.8/dist-packages (from black&lt;22.0,&gt;=21.5b0-&gt;pretty-confusion-matrix) (0.4.3)
Requirement already satisfied: platformdirs&gt;=2 in /usr/local/lib/python3.8/dist-packages (from black&lt;22.0,&gt;=21.5b0-&gt;pretty-confusion-matrix) (2.6.0)
Requirement already satisfied: typing-extensions&gt;=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from black&lt;22.0,&gt;=21.5b0-&gt;pretty-confusion-matrix) (4.4.0)
Requirement already satisfied: pyflakes&lt;2.4.0,&gt;=2.3.0 in /usr/local/lib/python3.8/dist-packages (from flake8&lt;4.0.0,&gt;=3.9.2-&gt;pretty-confusion-matrix) (2.3.1)
Requirement already satisfied: pycodestyle&lt;2.8.0,&gt;=2.7.0 in /usr/local/lib/python3.8/dist-packages (from flake8&lt;4.0.0,&gt;=3.9.2-&gt;pretty-confusion-matrix) (2.7.0)
Requirement already satisfied: mccabe&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.8/dist-packages (from flake8&lt;4.0.0,&gt;=3.9.2-&gt;pretty-confusion-matrix) (0.6.1)
Requirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (7.1.2)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (3.0.9)
Requirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (1.0.6)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (1.4.4)
Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (4.38.0)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (21.3)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (0.11.0)
Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (2.8.2)
Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;2.0.0,&gt;=1.3.4-&gt;pretty-confusion-matrix) (2022.6)
Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (0.10.2)
Requirement already satisfied: identify&gt;=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (2.5.10)
Requirement already satisfied: nodeenv&gt;=0.11.1 in /usr/local/lib/python3.8/dist-packages (from pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (1.7.0)
Requirement already satisfied: virtualenv&gt;=20.0.8 in /usr/local/lib/python3.8/dist-packages (from pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (20.17.1)
Requirement already satisfied: cfgv&gt;=2.0.0 in /usr/local/lib/python3.8/dist-packages (from pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (3.3.1)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.8/dist-packages (from pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (6.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nodeenv&gt;=0.11.1-&gt;pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (57.4.0)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&lt;4.0.0,&gt;=3.5.0-&gt;pretty-confusion-matrix) (1.15.0)
Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn&lt;0.12.0,&gt;=0.11.2-&gt;pretty-confusion-matrix) (1.7.3)
Requirement already satisfied: filelock&lt;4,&gt;=3.4.1 in /usr/local/lib/python3.8/dist-packages (from virtualenv&gt;=20.0.8-&gt;pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (3.8.2)
Requirement already satisfied: distlib&lt;1,&gt;=0.3.6 in /usr/local/lib/python3.8/dist-packages (from virtualenv&gt;=20.0.8-&gt;pre-commit&lt;3.0.0,&gt;=2.12.1-&gt;pretty-confusion-matrix) (0.3.6)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="338" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="S_pfBSBeKjlu" data-outputId="e47dd71a-d58f-450b-c49f-54cbaa77fbe1">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  apple_predictions <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  banana_predictions <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  false_positives <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  false_negatives <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  true_positives <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  true_negatives <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  iteration <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> image_index <span class="kw">in</span> banana_test:</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    image_path <span class="op">=</span> banana_path <span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>bananas[image_index]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(image_path).convert(<span class="st">&#39;L&#39;</span>))</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    input_vector <span class="op">=</span> image.flatten()            </span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    input_vector <span class="op">=</span> np.array(input_vector)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    output_vectors <span class="op">=</span> nn.forward(input_vector)  </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> output_vectors[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>]                         </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    iteration <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> prediction <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>      true_positives <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>      banana_predictions.append(<span class="dv">1</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>      false_negatives <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>      banana_predictions.append(<span class="dv">0</span>)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">&lt;</span> <span class="dv">4</span>:</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> verbose:</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        show_image(<span class="dv">1</span>,image_index)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Correct class: </span><span class="sc">{</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, prediction: </span><span class="sc">{</span>output_vectors[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>  iteration <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> image_index <span class="kw">in</span> apple_test:</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>      iteration <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>      image_path <span class="op">=</span> apple_path <span class="op">+</span> <span class="st">&quot;/&quot;</span><span class="op">+</span>apples[image_index]</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>      image <span class="op">=</span> np.asarray(Image.<span class="bu">open</span>(image_path).convert(<span class="st">&#39;L&#39;</span>))</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>      input_vector <span class="op">=</span> image.flatten()</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>      input_vector <span class="op">=</span> np.array(input_vector)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>      output_vectors <span class="op">=</span> nn.forward(input_vector)  </span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>      prediction <span class="op">=</span> output_vectors[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>]                           </span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> prediction <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>        apple_predictions.append(<span class="dv">1</span>)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>        false_positives <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>        true_negatives <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>        apple_predictions.append(<span class="dv">0</span>)</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> iteration <span class="op">&lt;</span> <span class="dv">4</span>:</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>          show_image(<span class="dv">0</span>,image_index)</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>          <span class="bu">print</span>(<span class="ss">f&quot;Correct class: </span><span class="sc">{</span><span class="dv">0</span><span class="sc">}</span><span class="ss">, prediction: </span><span class="sc">{</span>output_vectors[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> verbose:</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> [<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(apple_test))] <span class="op">+</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(banana_test))]</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> np.array(y_test)</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>    predic <span class="op">=</span> []</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> banana_predictions:</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>      predic.append(i)</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> apple_predictions:</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>      predic.append(i)</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>    predic <span class="op">=</span> np.array(predic)</span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>    pp_matrix_from_data(y_test, predic)</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="kw">not</span> verbose:</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> banana_predictions, apple_predictions</span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>test()</span></code></pre></div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-333-2a998eaf377d&gt;:3: RuntimeWarning: overflow encountered in exp
  return 1/(1+np.exp(-z))
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/84bdcc14d0a94c217ca2ac1a800451191a1661ab.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 1, prediction: 0.9866637625904632
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/bddd4c9a295e6f52ca2a5a7f0c323e39c4ed76a4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 1, prediction: 0.9866637625904632
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/2c2753a37e0003a109a50ebeff74802a2dba1480.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 1, prediction: 0.9866637625904632
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/e6ac109583a1a1d4fa18972812f4e8bbb0c5ef32.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 0, prediction: 3.981134679474438e-05
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/704a33b083d3c6a396c6811d04b7ef77a8b0ffd1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 0, prediction: 3.981134679474438e-05
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/f6834059cbc0061d3b985cda6fb078f8727a90b5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 0, prediction: 0.033720911826267394
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:200: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:201: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:203: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:204: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/ab2f33755d4c186a3dfffa62873fa10c4e83663a.png" /></p>
</div>
</div>
<div class="cell markdown" id="2WrM2K0hvOKA">
<p>The above results show 3 example predictions from each class, as well as a confusion matrix. The confusion matrix shows true and false positives in the top row and false and true negatives on the bottom row. (Class A is apples, class B is bananas). The accuracy (% correct / total samples) is shown as the green number in the bottom right square.</p>
<p>Most of the time this will have a pretty high false positive or false negative rate, which shows that solving an image recongnition problem like this would take a lot more than 10 epochs with our current neural network, and might never converge to a good accuracy.</p>
<p><br></p>
<p>So how do we make it better? Let's try adding more hidden layer neurons to allow it to recognize more complicated patterns:</p>
<p>(Be patient, it'll take longer to train with 24 neurons)</p>
</div>
<div class="cell code" data-execution_count="339" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="a5VlJthH-GlV" data-outputId="d56789bb-4f08-4b3c-b3a7-a364629575d2">
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>nn <span class="op">=</span> NeuralNetwork(<span class="dv">10000</span>, num_hidden_layer_neurons<span class="op">=</span><span class="dv">24</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    train()</span></code></pre></div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-333-2a998eaf377d&gt;:3: RuntimeWarning: overflow encountered in exp
  return 1/(1+np.exp(-z))
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>[&#39;0 : 0.14252297993229324&#39;, &#39;1 : 0.8538517967309939&#39;]
[&#39;0 : 0.05420639018119522&#39;, &#39;1 : 0.9058127941053945&#39;]
[&#39;0 : 0.03932112732206836&#39;, &#39;1 : 0.6456247905489407&#39;]
[&#39;0 : 0.03272425183653688&#39;, &#39;1 : 0.6043052995507142&#39;]
[&#39;0 : 0.029056700239931104&#39;, &#39;1 : 0.587716617667971&#39;]
[&#39;0 : 0.02631606749444276&#39;, &#39;1 : 0.5803535927221032&#39;]
[&#39;0 : 0.02407231629927887&#39;, &#39;1 : 0.5774708570112234&#39;]
[&#39;0 : 0.022204302808827275&#39;, &#39;1 : 0.5772189405123404&#39;]
[&#39;0 : 0.020653102291246797&#39;, &#39;1 : 0.578630564025099&#39;]
[&#39;0 : 0.019363675644009128&#39;, &#39;1 : 0.5810957571789153&#39;]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="340" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="-2gsPu-P-_-h" data-outputId="30799af4-e486-41c6-9835-60b670d2d05a">
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>test()</span></code></pre></div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-333-2a998eaf377d&gt;:3: RuntimeWarning: overflow encountered in exp
  return 1/(1+np.exp(-z))
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/84bdcc14d0a94c217ca2ac1a800451191a1661ab.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 1, prediction: 0.9997123402146794
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/bddd4c9a295e6f52ca2a5a7f0c323e39c4ed76a4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 1, prediction: 0.9997123402146794
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/2c2753a37e0003a109a50ebeff74802a2dba1480.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 1, prediction: 0.8799368901389061
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/e6ac109583a1a1d4fa18972812f4e8bbb0c5ef32.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 0, prediction: 0.046708079704958064
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/704a33b083d3c6a396c6811d04b7ef77a8b0ffd1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 0, prediction: 0.046708079704958064
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/f6834059cbc0061d3b985cda6fb078f8727a90b5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Correct class: 0, prediction: 0.8799368901389061
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:200: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:201: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:203: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:204: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/75c3714bd21a4e467440bfa560111fbaeffe340d.png" /></p>
</div>
</div>
<div class="cell markdown" id="uemBkyIUCjLe">
<p>Is the accuracy better than it was with 3 neurons? Due to the random sampling of a small dataset and only ten epochs, the 24 neuron net may have performed better or worse when you ran it just now. But if its performance randomly fluctuates, how do we know if 24 hidden layer neurons actually made it better?</p>
<p><br></p>
<p>The answer is k-fold cross validation. We run it k times, each time taking a different random sampling to be the testing data, and compare the results at the end. Do to the law of large numbers, the higher k is, the closer the reported performance we see will be to the actual performance. This lets us comare the two nets.</p>
<p>10-fold cross validation is standard but that took about 10 minutes to run, so let's do 3-fold instead. You can always feel free to change the k value to 10 if you have the patience.</p>
</div>
<div class="cell code" data-execution_count="341" id="0XW9kYruEu1T">
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_validate(apple_predictions,banana_predictions):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> [<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(apple_predictions))] <span class="op">+</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(banana_predictions))]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> np.array(y_test)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    predic <span class="op">=</span> []</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> banana_predictions:</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>      predic.append(i)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> apple_predictions:</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>      predic.append(i)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    predic <span class="op">=</span> np.array(predic)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    pp_matrix_from_data(y_test, predic)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="342" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="pJL0pY1pBN6w" data-outputId="a4dbf286-980b-4a2c-d1a5-fb2c653f0fd7">
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># change this number to 10 for 10-fold cross validation</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>apple_predictions <span class="op">=</span> []</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>banana_predictions <span class="op">=</span> []</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TRAINING THE 3 NEURON NET&quot;</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>  apple_test, banana_test <span class="op">=</span> generate_test_set(apples,bananas)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>  nn <span class="op">=</span> NeuralNetwork(<span class="dv">10000</span>, num_hidden_layer_neurons<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f&quot;K-fold number: </span><span class="sc">{i}</span><span class="ss">&quot;</span>)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    train()</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>  ap, bp <span class="op">=</span> test(verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>  apple_predictions <span class="op">+=</span> ap</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>  banana_predictions <span class="op">+=</span> bp</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;3 HIDDEN LAYER NEURONS </span><span class="sc">{k}</span><span class="ss"> FOLD CROSS VALIDATION&quot;</span>)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>cross_validate(apple_predictions,banana_predictions)</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TRAINING THE 24 NEURON NET&quot;</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>apple_predictions <span class="op">=</span> []</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>banana_predictions <span class="op">=</span> []</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>  apple_test, banana_test <span class="op">=</span> generate_test_set(apples,bananas)</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>  nn <span class="op">=</span> NeuralNetwork(<span class="dv">10000</span>, num_hidden_layer_neurons<span class="op">=</span><span class="dv">24</span>)</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f&quot;K-fold number: </span><span class="sc">{i}</span><span class="ss">&quot;</span>)</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>    train()</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>  ap, bp <span class="op">=</span> test(verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>  apple_predictions <span class="op">+=</span> ap</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>  banana_predictions <span class="op">+=</span> bp</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;24 HIDDEN LAYER NEURONS </span><span class="sc">{k}</span><span class="ss"> FOLD CROSS VALIDATION&quot;</span>)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>cross_validate(apple_predictions,banana_predictions)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>TRAINING THE 3 NEURON NET
Sequestering 98 pictures for testing
K-fold number: 0
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-333-2a998eaf377d&gt;:3: RuntimeWarning: overflow encountered in exp
  return 1/(1+np.exp(-z))
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>[&#39;0 : 0.2762928373456612&#39;, &#39;1 : 0.6044776153245577&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
[&#39;0 : 0.47740772857276703&#39;, &#39;1 : 0.6044776237234728&#39;]
Sequestering 98 pictures for testing
K-fold number: 1
[&#39;0 : 0.815990268543964&#39;, &#39;1 : 0.8584374781780054&#39;]
[&#39;0 : 0.8356234024058159&#39;, &#39;1 : 0.8719286742236726&#39;]
[&#39;0 : 0.4572671976022516&#39;, &#39;1 : 0.5604090531054126&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
[&#39;0 : 0.4572671976022515&#39;, &#39;1 : 0.5604090531054123&#39;]
Sequestering 98 pictures for testing
K-fold number: 2
3 HIDDEN LAYER NEURONS 3 FOLD CROSS VALIDATION
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:200: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:201: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:203: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:204: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/c941cdb4ad9ca845bbbcb6a77f1dc23410e82e4a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>TRAINING THE 24 NEURON NET
Sequestering 98 pictures for testing
K-fold number: 0
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-333-2a998eaf377d&gt;:3: RuntimeWarning: overflow encountered in exp
  return 1/(1+np.exp(-z))
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>[&#39;0 : 0.015897638203961128&#39;, &#39;1 : 0.8837920941939242&#39;]
[&#39;0 : 0.011181963569806642&#39;, &#39;1 : 0.8506986245474423&#39;]
[&#39;0 : 0.008358892205922792&#39;, &#39;1 : 0.8841616526146037&#39;]
[&#39;0 : 0.004626280901998549&#39;, &#39;1 : 0.8353654557557689&#39;]
[&#39;0 : 0.001621403588749626&#39;, &#39;1 : 0.8560351991927619&#39;]
[&#39;0 : 0.001197085558016726&#39;, &#39;1 : 0.8725066164282475&#39;]
[&#39;0 : 0.0009474414291950404&#39;, &#39;1 : 0.8843599405994722&#39;]
[&#39;0 : 0.0007793522986266006&#39;, &#39;1 : 0.8931660480163833&#39;]
[&#39;0 : 0.0006580810976505387&#39;, &#39;1 : 0.9000003316606398&#39;]
[&#39;0 : 0.0005664552394261073&#39;, &#39;1 : 0.9055209108597871&#39;]
Sequestering 98 pictures for testing
K-fold number: 1
Sequestering 98 pictures for testing
K-fold number: 2
[&#39;0 : 0.03027309126318473&#39;, &#39;1 : 0.29857577310293576&#39;]
[&#39;0 : 0.06841985170222789&#39;, &#39;1 : 0.3178553467505955&#39;]
[&#39;0 : 0.054211402529509976&#39;, &#39;1 : 0.3954166037414903&#39;]
[&#39;0 : 0.03919767116292961&#39;, &#39;1 : 0.37646990037938793&#39;]
[&#39;0 : 0.03182126485163686&#39;, &#39;1 : 0.3689475394829905&#39;]
[&#39;0 : 0.027238829349346887&#39;, &#39;1 : 0.3655423921787539&#39;]
[&#39;0 : 0.024005662333992494&#39;, &#39;1 : 0.36376008070758636&#39;]
[&#39;0 : 0.021553520775239948&#39;, &#39;1 : 0.3626854360071629&#39;]
[&#39;0 : 0.019610061491897222&#39;, &#39;1 : 0.3619580043394968&#39;]
[&#39;0 : 0.018024253120573555&#39;, &#39;1 : 0.3614241608082951&#39;]
24 HIDDEN LAYER NEURONS 3 FOLD CROSS VALIDATION
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:200: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:201: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:203: MatplotlibDeprecationWarning: 
The tick1On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick1line.set_visible instead.
  t.tick1On = False
/usr/local/lib/python3.8/dist-packages/pretty_confusion_matrix/pretty_confusion_matrix.py:204: MatplotlibDeprecationWarning: 
The tick2On function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use Tick.tick2line.set_visible instead.
  t.tick2On = False
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_a74ce979edff49b49ba28f671348d56d/3329aa929d5397cc820ba43e452035525596b751.png" /></p>
</div>
</div>
<div class="cell markdown" id="t_vSuAGkOx6e">
<p>Comparing the two, you'll see that the 24 neuron network actually did <em>worse</em> than the 3 neuron network. This might be surprising, but it demonstrates that image recognition is much more complicated than something like the XOR problem, and can't be solved with a single layer vanilla neural network like this one. The reason is that we only have one hidden layer. Deep neural networks take a lot longer to train, but having multiple layers allows them to solve much more complicated problems. Furthermore, Convolutional Neural Networks (CNNs) use edge detecting kernals and other tricks to filter out the pixels that aren't as important.</p>
<p>Stay tuned for a tutorial on deep learning and CNNs from scratch!</p>
</div>
</body>
</html>
